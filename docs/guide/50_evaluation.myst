---
file_format: mystnb
kernelspec:
  name: python3
---

# Evaluation

This section covers the evaluation functionality in `delaynet`, which includes methods for comparing original and reconstructed networks.

## Network Evaluation Metrics

`delaynet` provides a function `roc_auc_rank_c()` that calculates three important metrics for evaluating the quality of network reconstruction:

```{code-cell}
import numpy as np

original_network = np.array([[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]])
weight_matrix = np.array([[0.1, 0.9, 0.91, 0.05], [0.9, 0.1, 0.05, 0.92],
                         [0.91, 0.05, 0.1, 0.93],
                         [0.05, 0.92, 0.05, 0.65]])
reconstructed_network = np.array([[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 0, 1]])
```

```{code-cell}
from delaynet.evaluation import roc_auc_rank_c
from matplotlib import pyplot as plt

# Example: Evaluate a reconstructed network
roc, auc_value, rank_c = roc_auc_rank_c(
    orig_net=original_network,      # Original network (adjacency matrix)
    weight_mat=weight_matrix,       # Weights of the original network
    rec_net=reconstructed_network   # Reconstructed network
)
print(f"AUC Value: {auc_value}, Rank C: {rank_c}")
plt.figure(figsize=(9, 6), dpi=300)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(roc[:, 0], roc[:, 1], label=f'ROC curve (AUC = {auc_value:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()
```

### Receiver Operating Characteristic (ROC) Curve

The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. In the context of network reconstruction, it shows the trade-off between:

- True Positive Rate (TPR): The proportion of actual connections that are correctly identified
- False Positive Rate (FPR): The proportion of non-connections that are incorrectly identified as connections

The ROC curve is created by plotting the TPR against the FPR at various threshold settings. The `roc_auc_rank_c()` function returns the ROC curve as a numpy array with shape (n_points, 2), where each row contains (FPR, TPR) values.

### Area Under Curve (AUC)

The Area Under the ROC Curve (AUC) is a measure of the overall performance of a network reconstruction method. It represents the probability that a randomly chosen existing connection is ranked higher than a randomly chosen non-existing connection.

- AUC = 1.0: Perfect reconstruction
- AUC = 0.5: Random reconstruction (no better than random guessing)
- AUC < 0.5: Worse than random guessing

The `roc_auc_rank_c()` function calculates the AUC using scikit-learn's `auc` function.

### Spearman Rank Correlation Coefficient (rank_c)

The Spearman rank correlation coefficient measures the strength and direction of association between the original network weights and the reconstructed network weights. It assesses how well the reconstructed network preserves the relative strengths of connections in the original network.

- rank_c = 1.0: Perfect correlation (connections are ranked in the same order)
- rank_c = 0.0: No correlation
- rank_c = -1.0: Perfect negative correlation

The `roc_auc_rank_c()` function calculates the absolute value of the Spearman rank correlation coefficient using SciPy's `spearmanr` function.

## Visualizing Evaluation Results

You can visualise the ROC curve using matplotlib:

```python
import matplotlib.pyplot as plt

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(roc[:, 0], roc[:, 1], 'b-', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', linewidth=1)  # Diagonal line (random guess)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title(f'ROC Curve (AUC = {auc_value:.3f}, Rank Correlation = {rank_c:.3f})')
plt.grid(True)
plt.show()
```

## Comparing Multiple Reconstruction Methods

You can use these metrics to compare the performance of different network reconstruction methods:

```python
# Compare multiple reconstruction methods
methods = ['Method A', 'Method B', 'Method C']
auc_values = []
rank_c_values = []

for method_name, reconstructed_net in zip(methods, reconstructed_networks):
    _, auc, rank_c = roc_auc_rank_c(original_network, weight_matrix, reconstructed_net)
    auc_values.append(auc)
    rank_c_values.append(rank_c)
    print(f"{method_name}: AUC = {auc:.3f}, Rank Correlation = {rank_c:.3f}")
```

By comparing these metrics across different methods, you can determine which approach provides the most accurate network reconstruction for your specific data.
